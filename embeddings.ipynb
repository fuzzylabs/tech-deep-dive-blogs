{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Embeddings with HuggingFace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, ViTMAEModel, PreTrainedModel\n",
    "from PIL import Image\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from datasets import load_dataset, Dataset\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/Users/christophernorman/.cache/huggingface/datasets/imagefolder/default-073897f9ede2efb8/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47889ddf32d43c78f5f909a8e03acb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_dataset = load_dataset(\"imagefolder\", data_dir=\"data/\")\n",
    "\n",
    "# Remove train/test segmentation\n",
    "image_dataset = image_dataset['train']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the image processor and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTMAEModel: ['decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.mask_token', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.weight']\n",
      "- This IS expected if you are initializing ViTMAEModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTMAEModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "model = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img: Image, image_processor: AutoImageProcessor) -> torch.tensor:\n",
    "    \"\"\"Preprocess image using a HuggingFace auto image processor.\n",
    "\n",
    "    Args:\n",
    "        img (Image): Pillow image\n",
    "        image_processor (AutoImageProcessor): HuggingFace image processor\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: Preprocessed image as a Torch tensor\n",
    "    \"\"\"\n",
    "    # Convert image to RGB if it is not already.\n",
    "    img = img.convert(\"RGB\")\n",
    "    \n",
    "    return image_processor(images = img, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/christophernorman/.cache/huggingface/datasets/imagefolder/default-073897f9ede2efb8/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-7c14fc7d2ae45fdd.arrow\n"
     ]
    }
   ],
   "source": [
    "# Process images using HuggingFace processor\n",
    "image_dataset = image_dataset.map(lambda x: {\"preprocessed_image\": preprocess_image(x['image'], image_processor=image_processor)})\n",
    "\n",
    "# Set dataset format to PyTorch\n",
    "image_dataset = image_dataset.with_format(\"pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the images through the model and extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(model, preprocessed_image: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Passes a preprocessed image through a pretrained embedding model.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): Pretrained HuggingFace PyTorch embedding model.\n",
    "        preprocessed_image (torch.Tensor): Preprocessed image as a PyTorch Tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Embedding vector shape (1, 768) as a Tensor\n",
    "    \"\"\"\n",
    "    embedding = model(**preprocessed_image).last_hidden_state[:, 0]\n",
    "\n",
    "    return np.squeeze(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/christophernorman/.cache/huggingface/datasets/imagefolder/default-073897f9ede2efb8/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-b22d65f36126e755.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'name', 'preprocessed_image', 'embedding'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset = image_dataset.map(lambda img: {\"embedding\": create_embedding(model, img[\"preprocessed_image\"])})\n",
    "image_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print structure of an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7841e-01,  7.8200e-02, -2.8591e-01,  1.2317e-01, -9.1169e-01,\n",
       "        -9.8419e-01,  1.6852e-01,  1.8547e-01,  2.1094e-01, -3.4289e-01,\n",
       "        -4.1250e-02,  8.2595e-02,  3.2813e-01,  6.8463e-03,  2.3943e-03,\n",
       "         1.2862e-02,  8.1512e-02, -2.3367e-01, -1.6421e-01,  4.2300e+00,\n",
       "         1.4997e-01,  9.9038e-02, -3.6799e-01, -8.9632e-03, -7.6403e-02,\n",
       "        -2.1601e-01, -4.9117e-02,  3.2531e-02, -1.7313e-01, -1.9451e-01,\n",
       "         1.1379e-01, -2.0484e-02,  1.9072e-01,  1.7569e-01, -1.2619e-01,\n",
       "         3.4783e-02,  5.0942e-04,  6.6755e-02, -2.2290e-01, -2.6659e-01,\n",
       "         1.0214e-01, -6.0784e-02, -1.8836e-01, -3.4465e-01, -2.2797e-01,\n",
       "        -5.6797e-01,  1.1735e-01, -1.0172e-03, -4.0231e-01,  8.1782e-02,\n",
       "        -7.9657e-02, -3.0384e-02,  1.6060e-01, -1.1649e-01, -3.5328e-01,\n",
       "         6.8403e-02, -1.8808e-01,  1.1057e-01,  1.2938e-01, -8.5726e-02,\n",
       "         2.0943e-01, -2.3959e-01,  3.0581e-02,  3.4867e-01, -2.1898e-01,\n",
       "         8.4451e-02, -1.6469e-01, -1.4552e-01, -7.3528e-02,  1.2734e-01,\n",
       "         1.8665e-01, -1.1801e-01,  9.5080e-02,  7.2665e-03,  2.0474e-01,\n",
       "        -2.1614e-01, -2.1704e-01,  1.4032e-02,  4.4690e-02, -1.5659e-01,\n",
       "        -1.5698e-01, -9.1205e-03,  2.5203e-01,  6.9758e-02,  1.1723e-02,\n",
       "         3.0185e-01, -6.9981e-02,  1.0269e-01, -1.1124e-01, -4.8443e-02,\n",
       "        -7.4424e-02,  2.3783e-01, -5.0206e-01,  3.9970e-01, -5.0251e-01,\n",
       "        -1.5154e-01,  9.3444e-02,  1.1590e-01, -1.2942e-01, -6.7748e-02,\n",
       "        -3.3971e-02, -1.5284e-01, -7.8760e-02, -5.7413e+00,  3.1118e-01,\n",
       "        -8.3341e-02,  1.9377e-01,  2.2544e-01, -4.9609e-02, -2.2048e-01,\n",
       "        -1.4251e-01,  1.1908e-01, -7.3557e-02,  2.0541e-01, -3.5533e-01,\n",
       "         2.5859e-01, -4.1853e-01, -2.3281e-01,  3.8083e-01, -3.1026e-02,\n",
       "         7.5180e-02, -6.3656e-02,  1.0812e-01, -6.8683e-02,  1.1237e-01,\n",
       "        -1.8995e-01,  2.4796e-01,  6.4153e-02,  1.5989e-01, -1.2520e-01,\n",
       "        -9.0656e-02,  8.2043e-02,  2.2249e-01, -2.0253e-02,  1.6979e-01,\n",
       "        -2.4285e-01,  2.6720e-01,  9.4470e-03,  2.4273e-01, -4.3886e-02,\n",
       "        -5.8826e-02, -4.6969e-02,  3.3030e-02,  2.0992e-01, -2.0725e-02,\n",
       "        -2.0175e-01, -1.0013e-02, -2.1002e-01,  3.2958e-03, -2.1331e-01,\n",
       "        -2.6679e-01,  4.9660e-02,  4.3557e-01,  1.4592e-01, -5.7816e-02,\n",
       "         3.5027e-01, -5.8974e-02,  2.9486e-03, -1.6702e-01,  2.6120e-02,\n",
       "         6.8613e-02, -1.0352e-02,  2.0756e-01, -1.5728e-01,  1.2642e-01,\n",
       "        -1.3839e-01,  2.8009e-01,  4.5596e-02, -1.2077e-01, -9.0750e-02,\n",
       "        -3.7536e-01, -7.3554e-03,  2.9171e-01, -6.0645e-02, -3.9276e-01,\n",
       "        -1.5212e-01, -9.7683e-02,  2.2430e-01, -4.5868e-02, -3.4470e-02,\n",
       "        -5.0933e-02,  1.8103e-01, -1.7933e-01, -2.3038e-01, -3.8941e-02,\n",
       "         3.9927e-01,  7.4198e-03,  3.7496e-02, -2.4976e-01, -1.0093e-01,\n",
       "        -1.3364e-01,  3.2292e-01,  1.1693e-01, -6.5676e-02, -1.2816e-02,\n",
       "        -2.7886e-01, -3.2915e-01, -3.8648e-01,  6.1960e-02,  2.7247e-03,\n",
       "        -2.7095e-01, -3.6364e-02, -3.2448e-01, -1.8367e-01,  6.3096e-01,\n",
       "         1.8205e-01,  3.4730e-02,  2.1368e-01,  3.6718e-01, -6.3989e-02,\n",
       "        -1.5356e-01,  1.5987e-01,  1.4152e-01,  5.6475e-01,  1.2967e-01,\n",
       "         1.3022e-01,  1.1142e-01, -1.1495e-01,  6.9554e-01,  7.6020e-01,\n",
       "        -8.6316e-02,  2.1924e-01, -4.1629e-02,  1.0448e+00,  1.1315e-02,\n",
       "         1.4864e-02, -2.5430e-02,  3.9474e-02, -3.7386e-02, -3.2470e-01,\n",
       "        -8.8977e-02, -4.5447e-02, -1.4970e-01, -8.0661e-02,  3.2125e-02,\n",
       "        -2.3112e-01, -4.5066e-02,  1.6630e-02, -2.0068e-01,  1.4117e-01,\n",
       "        -3.0714e-01, -1.7525e-01, -1.6068e-01, -8.5724e-02,  7.9202e-01,\n",
       "        -1.1693e-01, -1.5615e-01, -1.2831e-01,  4.2332e-02, -6.1316e-01,\n",
       "         8.5702e-02,  2.1398e-02, -2.0980e-01, -1.0402e-01,  3.2437e-01,\n",
       "        -1.4489e-02,  8.0423e-03, -1.0322e-01,  7.3980e-02,  7.3168e-02,\n",
       "        -2.0851e+00,  5.6058e-02,  1.5613e-01, -1.3971e-01,  8.2269e-01,\n",
       "        -2.7005e-01,  1.3294e-01, -4.3916e-01,  2.2105e-01, -6.5811e-02,\n",
       "         1.2378e-02,  9.0163e-02, -2.6816e-01, -3.4452e-02, -3.6086e+00,\n",
       "        -3.3712e-01, -2.6630e-01, -6.1397e-02, -1.5095e-01, -3.4357e-02,\n",
       "         7.1392e-02,  9.2255e-02,  7.3132e-02, -4.8892e-01, -6.4208e-02,\n",
       "         7.4149e-02,  5.2660e-01,  1.2315e-02,  6.0144e-02,  7.0427e-03,\n",
       "        -3.3947e-02, -9.8169e-02,  5.4420e-02,  2.8306e-01,  2.8659e-01,\n",
       "        -5.9220e-01,  3.7935e-02, -1.5112e+00, -8.4889e-02, -2.6257e-01,\n",
       "         1.1513e-01, -2.7855e-02,  2.7635e-01,  1.7526e-01, -2.5988e+00,\n",
       "         1.0540e-02,  2.5218e-01,  4.5171e-02,  1.8699e-01, -6.5261e-02,\n",
       "         7.5386e-02, -1.4666e-02,  1.6658e-01,  4.6165e-02,  1.2358e-01,\n",
       "        -2.5681e-01,  1.0980e-01,  3.3175e-02,  2.3649e-01,  2.4202e-01,\n",
       "        -6.6360e-01,  8.2477e-02, -1.0607e-01, -1.3704e-01,  5.9981e-02,\n",
       "         6.5409e-01,  1.5851e-02, -3.8987e-01,  1.6569e-01,  1.3595e-01,\n",
       "        -2.6861e-01, -3.7381e-01, -1.2786e-01,  1.3364e-02, -2.3706e-01,\n",
       "        -1.5112e-01,  4.0400e-02,  2.6280e-02, -5.8537e-02, -5.1068e-02,\n",
       "         1.8162e-03,  2.1432e-01, -4.5788e-02,  2.0385e-01, -8.6366e-02,\n",
       "        -1.5382e-02, -2.3081e-02,  6.4528e-03,  2.2795e-03,  4.2279e-03,\n",
       "        -2.2128e-01,  2.0813e-01,  4.5998e-02,  2.1086e-01,  1.3228e-01,\n",
       "        -2.1949e-01,  3.3946e-01, -1.2201e-01, -2.3537e-03, -1.1686e-01,\n",
       "         4.7853e-02,  2.6820e-02,  2.2398e-01,  2.5521e-01, -8.8924e-02,\n",
       "        -1.2799e-02, -1.1696e-01, -7.6371e-02,  3.4724e-01, -1.1067e-01,\n",
       "         1.9065e-01, -1.6906e-02, -1.6911e-01,  1.7574e-01,  4.1738e-02,\n",
       "        -7.1903e-02,  6.2191e-03,  2.8605e-01, -3.7738e-02, -4.3700e-01,\n",
       "         5.7996e-02, -3.1065e-01,  4.9882e-02,  1.0763e-01,  8.5990e-01,\n",
       "        -1.7578e-01, -4.5572e-01,  9.6700e-01,  3.5679e-01, -5.2088e-02,\n",
       "        -6.7232e-02,  6.7331e-02,  1.1721e-01, -5.6561e-02,  2.1427e+00,\n",
       "         9.1951e-02,  2.1298e-01, -1.8564e-02, -6.6765e-02,  2.5563e-02,\n",
       "        -1.1404e-01,  1.3445e-01, -1.5704e-01, -1.7753e-01,  6.7925e-02,\n",
       "         3.4283e-03, -2.6135e-01,  5.9631e-02, -3.0695e-01, -8.1262e-03,\n",
       "        -9.5447e-03, -1.7250e-01, -7.4353e-02,  1.5229e-02,  2.3039e-02,\n",
       "         1.1763e-01, -3.5577e-01, -4.9367e-01,  1.0071e-01,  2.3608e-01,\n",
       "        -9.8249e-02, -1.8784e-01,  5.0643e-02,  2.8416e-01, -1.5329e-01,\n",
       "        -6.5504e-02,  2.5851e-03, -2.0040e-01,  2.6276e-02,  2.5564e-02,\n",
       "         3.0784e-01, -1.1789e-01,  1.3104e-01, -1.5608e-01, -1.9067e-01,\n",
       "        -1.8058e-03,  1.5784e-01, -1.5357e-02, -2.2387e-01, -5.1888e-02,\n",
       "         1.5809e-01, -8.9722e-02,  1.6701e-01, -1.0750e-01,  7.6084e-02,\n",
       "        -5.7909e-02,  3.2910e-01,  2.2926e-01,  1.6031e-01,  1.3626e-01,\n",
       "        -9.8081e-02,  1.7578e-01, -1.2679e-01,  5.7222e-01, -2.5860e-01,\n",
       "         1.9882e-01,  1.1284e-01,  1.5518e+00, -2.2814e-01, -2.8107e-02,\n",
       "         5.6908e-02,  4.2696e-02, -7.9761e-02,  2.9763e-02,  2.2661e-01,\n",
       "        -7.8986e-02, -3.3638e-02, -6.8067e-03,  1.4990e-01,  1.0846e-01,\n",
       "         1.4886e-01, -2.8000e-02,  3.0290e-01,  2.1526e-01,  1.9055e-02,\n",
       "         1.5921e-01,  2.1777e-01,  5.6671e-02, -1.7623e-01,  2.8133e-01,\n",
       "        -9.1452e-02, -2.5468e-01,  1.6917e-01,  1.3321e-01,  2.6212e-01,\n",
       "         1.4913e+00, -2.4060e-01,  1.9120e-01, -7.6888e-02,  4.6127e-01,\n",
       "        -1.8788e-01, -3.9674e-01,  7.5251e-02,  4.7598e-02,  8.1440e-02,\n",
       "         1.2369e-01,  4.0497e-02, -8.9263e-02, -2.9081e-02, -1.6566e-01,\n",
       "        -7.9698e-02,  1.1876e-01, -1.4810e-01, -2.0322e-01,  1.4816e-01,\n",
       "        -5.8468e-02,  1.1274e-01,  1.2561e-01,  2.4247e-01,  2.1386e-01,\n",
       "        -1.4515e-01, -4.8783e-02,  4.8090e-01, -2.1426e-02, -2.0179e-01,\n",
       "        -1.2317e-01,  1.2930e-01, -1.1316e-01, -3.9955e-02, -1.5110e-01,\n",
       "         2.5836e-01, -1.2281e-01,  9.0140e-02,  4.6037e-01, -2.1937e-01,\n",
       "         1.9805e-01,  2.8478e-01,  6.2919e-02, -3.4580e-01,  7.8945e-01,\n",
       "         1.7371e-01,  2.6672e-01,  2.1072e-01, -1.4990e-01,  1.3654e-02,\n",
       "         6.1444e-03,  3.7175e-02, -4.9269e-02,  2.5577e-01, -1.0400e-01,\n",
       "        -7.4691e-01,  5.6883e-02, -5.9402e-02,  1.6036e-01, -9.3125e-03,\n",
       "        -1.9654e-02,  1.0435e-02, -1.6374e-01, -6.3311e-02, -1.3762e-01,\n",
       "         2.3528e-02,  3.1419e-01,  3.5033e-01, -1.8158e-01, -1.7142e-02,\n",
       "         1.5876e-01, -1.0229e-01, -1.7525e-02, -2.2405e+00,  2.4142e-01,\n",
       "        -2.2522e-02,  6.4696e-02,  1.9604e-01, -6.0126e-01, -7.8166e-02,\n",
       "         3.2125e-01, -1.1046e-02, -1.7331e-01, -6.7423e-02, -1.8007e-01,\n",
       "        -2.0330e-01,  1.5021e-01, -5.2505e-02,  4.5976e-02,  4.0256e-02,\n",
       "         8.3678e-02, -3.3886e-02,  1.9054e-01, -8.7504e-01,  1.0038e+00,\n",
       "         4.1159e-01, -7.9391e-01,  2.2604e-01, -1.8456e-02, -4.0782e-02,\n",
       "        -2.7107e-01, -6.0411e-01, -3.7779e-01, -4.4762e-03, -1.6118e-01,\n",
       "         1.5034e-01,  8.3585e-02,  1.4492e-01,  4.0707e-01,  2.8246e-01,\n",
       "         1.1198e-01, -1.6378e-01,  4.1556e-01,  7.5804e-02, -4.0238e-02,\n",
       "        -2.1116e-02,  4.4809e-02, -2.4187e-01,  2.3686e-02,  2.0712e-01,\n",
       "        -4.0296e-02,  1.0695e+00, -2.6397e-01,  4.3971e-02, -8.8767e-02,\n",
       "         2.4529e-02, -4.6964e-02,  3.0969e-01, -1.1617e-01, -2.6025e-02,\n",
       "         9.4890e-02, -1.3778e-02,  2.7336e-01,  1.5099e-01, -6.2462e-02,\n",
       "         1.8431e-01,  1.0325e-01, -6.5186e-03,  7.3280e-02, -3.6747e-02,\n",
       "        -1.1491e-01,  4.3649e-01, -1.2794e-01, -1.8418e-01, -1.7468e-02,\n",
       "         5.1623e-02,  1.2033e-02, -5.2988e-01,  6.4643e-02, -4.8528e-02,\n",
       "         4.7800e-03,  4.8470e-02, -3.0766e-02,  1.5894e-01, -5.0329e-02,\n",
       "         1.2756e-01,  4.1911e-01,  1.4351e-01, -2.9724e-01,  7.1102e-01,\n",
       "        -2.5283e-01,  2.0393e-01,  3.1146e-01,  3.2930e-02, -1.7298e-01,\n",
       "        -3.0882e-01, -6.4538e-01, -1.1266e-01, -3.1228e-02, -1.9209e-02,\n",
       "         4.6769e-02, -4.2807e-02, -2.7644e-01, -2.2989e-01,  4.9762e-02,\n",
       "         2.0174e-01, -7.4674e-02, -5.1625e-02, -2.5163e-01,  1.0899e-01,\n",
       "        -1.5800e-01,  1.7202e-01, -4.6531e-01,  1.4708e-01, -1.2890e-01,\n",
       "        -1.7758e-03,  2.7965e-01, -8.5057e-02,  2.2522e-01, -1.9089e-02,\n",
       "         2.1357e-01, -6.4589e-01, -4.9243e-01,  3.4257e-01, -2.0630e-01,\n",
       "         2.1450e-01, -4.3191e-01, -1.0726e-01, -6.4084e-02, -1.3842e-01,\n",
       "        -2.0692e-01, -6.9224e-02, -1.6361e-01, -5.4527e-02, -6.6929e-02,\n",
       "         1.8629e-01,  9.0645e-01, -1.7065e-01,  1.7867e-01,  5.3392e-02,\n",
       "        -2.7454e-01,  9.8166e-02,  2.3776e-01, -1.9330e-01, -3.2784e-02,\n",
       "        -2.3893e-02, -3.4092e-01,  1.1383e-01, -2.5070e-01,  5.0759e-02,\n",
       "         2.0630e-01, -5.8661e-03,  1.8054e-01,  2.6582e-01,  8.0764e-02,\n",
       "        -1.2352e-02,  4.8418e-02,  4.2890e-02, -2.2404e-01, -3.6096e-01,\n",
       "        -1.1689e-01, -4.4199e-01,  6.4546e-02, -3.6372e-02, -1.0805e-02,\n",
       "         1.0095e-01,  1.1609e-01, -2.1079e-01, -2.4100e-01,  1.2428e-01,\n",
       "         1.4814e-02, -2.3484e-01, -4.6206e-02, -2.1096e-01,  3.0410e-02,\n",
       "         1.2117e-01, -2.5657e-01, -1.4765e-01, -2.1204e-01, -1.2012e-01,\n",
       "        -2.4257e-01,  4.3665e-02,  3.1635e-01,  7.6894e-02,  1.0395e-01,\n",
       "         1.6398e-01,  1.1866e-01, -1.1164e-01,  8.0837e-02,  1.1841e-01,\n",
       "        -1.1810e-01, -1.9773e-01,  2.1572e-02, -1.9868e-01,  4.9975e-01,\n",
       "        -5.4679e-02, -3.9103e-01,  1.7502e-02,  2.1811e-01,  3.6035e-01,\n",
       "        -6.1467e-02, -5.6768e-02, -1.0529e-01,  4.2769e-02, -1.6860e-01,\n",
       "        -9.9509e-02,  1.4280e-01,  2.5049e-01,  5.8560e-02, -6.3377e-02,\n",
       "         5.6106e-02, -2.0559e-01,  4.7209e-01])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset[0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset[0]['embedding'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export data to TSV file and metadata for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_to_tsv(image_dataset: Dataset):\n",
    "    \"\"\"Saves embeddings as tab seperated values to be used at https://projector.tensorflow.org/ .\n",
    "\n",
    "    Args:\n",
    "        image_dataset (datasets.Dataset): Dataset containing an embedding and a name column.\n",
    "    \"\"\"\n",
    "    # Save embedding values seperately\n",
    "    with open(\"embeddings.tsv\", \"w\") as file:\n",
    "        for row in image_dataset:\n",
    "            for i in row['embedding'].numpy():\n",
    "                file.write(f\"{i}\\t\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    # Save metadata\n",
    "    with open(\"metadata.tsv\", \"w\") as file:\n",
    "        for row in image_dataset:\n",
    "            file.write(f\"{row['name']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_to_tsv(image_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the saved csv files into the Embedding Projector here: https://projector.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "known-origin-nft-analytics-demo-suGx7Of8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "661c53f6326e21a512fad75b12dd87494d29b1194ad1d3ff51c5b1be1a2ce983"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
